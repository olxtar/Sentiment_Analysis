{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "571534dc",
   "metadata": {},
   "source": [
    "# 03. Notebook: Sentiment RNN\n",
    "\n",
    "영화 리뷰들을 분석하여 해당 리뷰가 긍정적 리뷰인지 / 부정적 리뷰인지 예측하는 모델만들기\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98eadab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\Sentiment_Analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cdd65b",
   "metadata": {},
   "source": [
    "## 01. Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e537e9e",
   "metadata": {},
   "source": [
    "### 1-0.Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee46d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read data from text files\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    labels = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7aebb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Review's characters: 33678267\n",
      "\n",
      "Review data sample: \n",
      " bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  \n",
      "homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school  work  or vote for the matter . most people think of the homeless as just a lost cause while worrying about things such as racism  the war on iraq  pressuring kids to succeed  technology  the elections  inflation  or worrying if they  ll be next to end up on the streets .  br    br   but what if y\n",
      "\n",
      "Label data sample: \n",
      " positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of Review's characters: {len(reviews)}\")\n",
    "print()\n",
    "print(f\"Review data sample: \\n {reviews[:2000]}\")\n",
    "print()\n",
    "print(f\"Label data sample: \\n {labels[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e82156",
   "metadata": {},
   "source": [
    "**Comment** :\n",
    "\n",
    "\n",
    "\n",
    "대략 3천3백만개의 글자로 이루어진 리뷰 텍스트 데이터가 있고\n",
    "\n",
    "\n",
    "각 리뷰에 대한 긍정/부정 정답, 즉 라벨데이터도 단순 텍스트덩어리로 되어있구나\n",
    "\n",
    "\n",
    "대충 리뷰를 읽어보니 Airport77이라는 영화의 리뷰인거같고 각 리뷰간의 구분은 `\\n`, 즉 줄바꿈으로 되는거같다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35e9ea",
   "metadata": {},
   "source": [
    "### 1-1. Data pre-processing\n",
    "\n",
    "텍스트데이터 -> 단어\n",
    "\n",
    "단어 -> 정수 (with Embedding layer)가 되어야 RNN에 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a5c947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 정제\n",
    "# 소문자로 변경, 구두점 삭제\n",
    "from string import punctuation\n",
    "print(punctuation)\n",
    "\n",
    "reviews = reviews.lower()\n",
    "all_text = ''.join([c for c in reviews if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70fa6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰간 구분하여 리스트만들기, 즉 [리뷰1, 리뷰2, 리뷰3...]\n",
    "reviews_split = all_text.split('\\n')\n",
    "\n",
    "# 딕셔너리 만들기 위해!\n",
    "# 리스트로 쪼개진 리뷰들을 띄어쓰기로 구분하여 다시 덩어리로 만들기\n",
    "all_text = ' '.join(reviews_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d83807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 리스트로 만들기 (리뷰간 구분없는)\n",
    "words = all_text.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34b3d01",
   "metadata": {},
   "source": [
    "### 1-2. Encoding the words\n",
    "\n",
    "Embedding Lookup Table을 사용하려면 입력이 '정수값'이어야 하므로 단어<-> 정수 Dictionary를 만들자 (빈도수순으로 정렬해서~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0c99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedcee7c",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> 추후에 패딩을 '0'으로 할것이므로 겹치지 않게 정수값 시작을 '1'부터로 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4c528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = { word: ii for ii, word in enumerate(vocab, start=1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9d7e2",
   "metadata": {},
   "source": [
    "리뷰 단어 -> 정수로 변환\n",
    "\n",
    "아까 위에서 만들어놨던 (정제된) 리뷰 리스트 `reviews_split` [리뷰1, 리뷰2, 리뷰3...]을 이제 정수로 변환해주자\n",
    "\n",
    "* [리뷰1, 리뷰2, ...] 리스트를 for loop로 돌린다\n",
    "* 하나씩 나오는 리뷰를 `split()`을 통해 단어가 나오게끔 for loop로 돌린다\n",
    "* 튀어나오는 단어들을 딕셔너리로 변환하여 리스트에 쌓는다\n",
    "* 결론적으로 [[리뷰1단어1, 리뷰1단어2,...], [리뷰2단어1, 리뷰2단어2], ....]와 같은 이중 리스트가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c1fad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = []\n",
    "\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e175c3",
   "metadata": {},
   "source": [
    "### 1-3. Encoding the labels\n",
    "\n",
    "아래처럼 각 리뷰에 대한 긍정/부정 라벨을 쪼개고, \n",
    "\n",
    "Positive는 1, Negative는 0으로 바꿔주자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2206de6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive\\nnegative\\npositive\\nnegative\\npositive\\nnegat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "764fcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "labels_split = labels.split('\\n')\n",
    "encoded_labels = [1 if label == 'positive' else 0 for label in labels_split]\n",
    "print(encoded_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163a071",
   "metadata": {},
   "source": [
    "### 1-4. Removing Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253eafb",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Additional pre-processing</span> -> `reviews_ints`는 아래와 같은 이차원(?) 리스트임\n",
    "\n",
    "[[리뷰1단어1, 리뷰1단어2,...],[리뷰2단어1, 리뷰2단어2,...],....]\n",
    "\n",
    "주의할 것이 각 리뷰마다의 길이, 즉 단어의 개수가 다르므로 이를 맞춰줄 필요가 있음\n",
    "\n",
    "따라서 단어개수가 하나도 없는 리뷰 or 너~무 많은 리뷰는 어느정도 제거해줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a89a3b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum review length : 0\n",
      "Maximum review length : 2514\n"
     ]
    }
   ],
   "source": [
    "# 각 리뷰의 길이 추출\n",
    "len_review = [ len(r) for r in reviews_ints]\n",
    "print(f\"minimum review length : {min(len_review)}\")\n",
    "print(f\"Maximum review length : {max(len_review)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda0d596",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> 길이가 0인, 즉 단어개수가 0인, 즉 아무것도 안써져있는 Review가 있나보네, 제일 긴 리뷰는 단어개수가 무려 2,514개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f1af5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이(단어개수)가 0이 아닌 리뷰의 인덱스만 추출하기\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "424c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이(단어개수)가 0이 아닌 리뷰의 라벨만 추출하기\n",
    "encoded_labels = [encoded_labels[ii] for ii in non_zero_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027010cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed1313",
   "metadata": {},
   "source": [
    "## 02. Padding Sequences\n",
    "\n",
    "* `reviews_ints` : 이중 리스트, 바깥은 리뷰단위, 속은 리뷰의 단어(정수)단위\n",
    "\n",
    "[[리뷰1단어1, 리뷰1단어2,], [리뷰2단어1,리뷰2단어2,...]....]\n",
    "\n",
    "그런데 각 리뷰마다 단어개수가 같을수없을거아니야? 우리는 RNN에 리뷰의 단어들을 Sequence느낌으로 넣어줘야하는데 이 Sequence Length가 동일해야겠지? 따라서\n",
    "\n",
    "1. `seq_length`보다 짧은 리뷰는 왼쪽부터 0으로 채워넣기 (패딩)\n",
    "2. `seq_length`보다 긴 리뷰는 `seq_length`까지만 남기고 나머지는 잘라내기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab3930",
   "metadata": {},
   "source": [
    "### 2-1. Padding or Truncate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0e892",
   "metadata": {},
   "source": [
    "아래와 같은 입출력을 먹고뱉는 함수를 만들자 `pad_features`\n",
    "\n",
    "입력 : `reviews_ints`, 즉 이중리스트, `seq_length`\n",
    "\n",
    "출력 : `features`, Review개수 x Sequence Length 사이즈의 2D Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97accb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    '''\n",
    "    Return features of reviews_ints, where each review is padded with 0's or\n",
    "    truncated to the input seq_length\n",
    "    '''\n",
    "    \n",
    "    review_li = []   # padded or truncated review 담아둘 리스트\n",
    "    for r in reviews_ints:\n",
    "        if len(r) < 200:\n",
    "            padding_n = seq_length - len(r)\n",
    "            padded_review = np.pad(r,\n",
    "                                   (padding_n, 0),\n",
    "                                   'constant',\n",
    "                                   constant_values = 0\n",
    "                                   )\n",
    "            review_li.append(padded_review)\n",
    "            \n",
    "        elif len(r) >= 200:\n",
    "            truncated_review = np.array(r[:seq_length])\n",
    "            review_li.append(truncated_review)\n",
    "            \n",
    "    # review_li는 길이 200의 review array들이 있는 '리스트'\n",
    "    # concatenate를 이용하여 일렬로 쭉 붙여주고\n",
    "    # reshape을 통해 (리뷰개수, seq_length)로 사이즈 만들어주기\n",
    "    features = np.concatenate(review_li).reshape(-1, seq_length)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc97c2",
   "metadata": {},
   "source": [
    "### 2-2. Function Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42e5e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]]\n",
      "\n",
      "(25000, 200)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "assert len(features) == len(reviews_ints)\n",
    "assert len(features[0]) == seq_length\n",
    "\n",
    "print(features[:10, :10])\n",
    "print()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1bfbb",
   "metadata": {},
   "source": [
    "**Comment** :\n",
    "\n",
    "Row는 각각의 리뷰(나중에 batch로 쓰일)들이고\n",
    "Column은 각 리뷰의 첫 10개의 단어(정수)\n",
    "\n",
    "총 25000개의 리뷰, 그리고 (각 리뷰의) 200개의 단어들이 `features`에 \n",
    "2D array로 할당되어있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1dcb29",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dee2713",
   "metadata": {},
   "source": [
    "## 03. Training, Validation, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f83a8e",
   "metadata": {},
   "source": [
    "* `features` : `(review개수, seq_lengh)` 사이즈의 2D Array\n",
    "* `encoded_labels` : `(, review개수)` 사이즈의 1D Array\n",
    "\n",
    "위 두개의 Array를 아래와 같은 비율로 나눠주자\n",
    "\n",
    "* Training : 80% `train_x` `train_y`\n",
    "* Validation : 10% `val_x` `val_y`\n",
    "* Test : 10% `test_x` `test_y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a7bec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_labels array로 변환시키기\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c78bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "\n",
    "# Training / Val+Test 나누기\n",
    "split_idx = int(len(features)*split_frac)  # 전체길이의 80%, 정수값 인덱스\n",
    "train_x, valtest_x = features[:split_idx], features[split_idx:]\n",
    "train_y, valtest_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "# Val Test 나누기\n",
    "split_idx_2 = int(len(valtest_x)*0.5)      # 전체길이의 20%의 절반, 즉 전체길이의 10%\n",
    "val_x, test_x = valtest_x[:split_idx_2], valtest_x[split_idx_2:]\n",
    "val_y, test_y = valtest_y[:split_idx_2], valtest_y[split_idx_2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29c7ccfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0474a11",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a70b2",
   "metadata": {},
   "source": [
    "## 04. DataLoaders and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d93b1af",
   "metadata": {},
   "source": [
    "* `train,valid,test_x` : x 데이터셋 (리뷰개수, 각리뷰 단어) 2D array\n",
    "* `train,valid,test_y` : y 데이터셋 (리뷰개수) 각 리뷰의 라벨 1D array\n",
    "\n",
    "데이터셋 준비는 완료되었다.\n",
    "\n",
    "`torch.utils.data.TensorDataset`을 통하여 x,y 텐서 데이터셋을 만들고\n",
    "(모아놓은) 데이터셋을 통하여 DataLoader를 만들자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106167d6",
   "metadata": {},
   "source": [
    "### 4-1. DataLoader 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b67a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x),\n",
    "                           torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x),\n",
    "                           torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x),\n",
    "                          torch.from_numpy(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd93e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff0dc",
   "metadata": {},
   "source": [
    "### 4-2. 1개의 Batch Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "baa0630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size: torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,   881,     6,   898],\n",
      "        [    0,     0,     0,  ...,   236,   115,    11],\n",
      "        [   54,     3,    18,  ...,     5,  3207,    42],\n",
      "        ...,\n",
      "        [  594, 12641,    47,  ...,  2255,  1522,     2],\n",
      "        [52491,   125,    48,  ...,    97,    28,    77],\n",
      "        [    1,   421,     4,  ...,     1,   219,   672]], dtype=torch.int32)\n",
      "\n",
      "Sample label size: torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        0, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print(f\"Sample input size: {sample_x.size()}\")\n",
    "print(f\"Sample input: \\n {sample_x}\")\n",
    "print()\n",
    "print(f\"Sample label size: {sample_y.size()}\")\n",
    "print(f\"Sample label: \\n {sample_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543254a4",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">[!]</span> input과 label의 Row는 하나의 리뷰를 의미하고 input에서의 Col은 Sequence Length, 즉 모델에 들어갈 단어의 개수를 의미한다. 또한 지금 모든 텐서가 int32 타입임을 인지하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59536742",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba8e69",
   "metadata": {},
   "source": [
    "## 05. Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dcf4c0",
   "metadata": {},
   "source": [
    "데이터셋 만들기 완료, DataLoader까지도 만들기 완료\n",
    "\n",
    "이제 드디어 '모델'을 만들 차례이다. 모델의 Layer는 아래와 같이 만들예정\n",
    "\n",
    "1. Embedding Layer : 단어 Token을 특정 크기의 Embedding vector로 변환해줌\n",
    "2. LSTM Layer : `hidden_state`의 size, 즉 LSTM cell의 노드수와 층과 층으로 이루어짐\n",
    "3. Fully-Connected Layer : LSTM Layer의 출력사이즈를 우리가 원하는 출력사이즈로 바꿔줌\n",
    "4. Sigmoid Activation Layer : 모든 출력들을 0~1사이의 값으로 바꿔줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacc653e",
   "metadata": {},
   "source": [
    "### 5-1. The Embedding Layer\n",
    "\n",
    "사용할 데이터셋(영화리뷰)에는 Unique Vocab이 무려 74,000개나 있고 이를 원핫인코딩, 즉 희소벡터로 만들어 사용하는 것은 매우 비효율적이므로 임베딩 레이어를 사용해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0185c",
   "metadata": {},
   "source": [
    "### 5-2. The LSTM Layer\n",
    "\n",
    "LSTM layer는 아래의 parameter를 받는다\n",
    "\n",
    "* `input_size` : 임베딩 레이어를 지나 만들어진 임베딩 벡터가 들어올 예정이므로 embedding dimension\n",
    "\n",
    "* `hidden_dim` : LSTM cell의 Hidden layer의 유닛개수, 보통 128, 256, 512로 함\n",
    "\n",
    "* `number of layers` : Hidden Layer의 개수\n",
    "\n",
    "* `dropout probability`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6846bc44",
   "metadata": {},
   "source": [
    "#### GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecc7d295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, Training on CPU\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if (train_on_gpu):\n",
    "    print(\"Training on GPU\")\n",
    "else:\n",
    "    print(\"No GPU available, Training on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbd267",
   "metadata": {},
   "source": [
    "### 5-3. Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63cc3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment Analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size,    # 우리가 넣어줄 입력 차원 = Unique vocab 개수\n",
    "                 output_size,\n",
    "                 embedding_dim, # 임베딩 벡터의 길이\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 drop_prob=0.5):\n",
    "    \n",
    "        super(SentimentRNN, self).__init__()\n",
    "        \n",
    "        # 왜 얘네만 self 선언하지?\n",
    "        # self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # Linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)    # input으로 들어온 배치덩어리의 Row수\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        \n",
    "        # Reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        # 출력에서 마지막 Column의 값, 즉 최종 타임스텝의 output값만 가져오기\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes hidden state\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create two new tensors with sizes(n_layers, batch_size, hidden_dim)\n",
    "        # Initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers,\n",
    "                                 batch_size,\n",
    "                                 self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers,\n",
    "                                 batch_size,\n",
    "                                 self.hidden_dim).zero_().cuda()\n",
    "                     )\n",
    "            \n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers,\n",
    "                                 batch_size,\n",
    "                                 self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers,\n",
    "                                 batch_size,\n",
    "                                 self.hidden_dim).zero_()\n",
    "                     )\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ccd432",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf71ad5e",
   "metadata": {},
   "source": [
    "## 06. Instantiate the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98046f36",
   "metadata": {},
   "source": [
    "* `vocab_size` : 우리가 모델에 넣는 데이터(최소단위)의 값의 개수는? Unique vocab개수! + 1 (for padding)\n",
    "* `output_size` : The number of class score, 영화리뷰에 대한 감정도\n",
    "* `embedding_dim` : Number of columns in the embedding lookup table; size of our embedding vector\n",
    "* `hidden_dim` : Number of units in the hidden layers of our LSTM class\n",
    "* `n_layers` : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60710fef",
   "metadata": {},
   "source": [
    "### 6-1. Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a969029",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab_to_int) +1\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size,\n",
    "                   output_size,\n",
    "                   embedding_dim,\n",
    "                   hidden_dim,\n",
    "                   n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3db97e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ad31",
   "metadata": {},
   "source": [
    "**Comment** :\n",
    "\n",
    "우리의 영화리뷰 데이터의 Unique한 Vocabulary의 개수는 74,072개임.\n",
    "\n",
    "여기에 Padding을 의미하는 '0'도 포함시켜서 총 74,073개\n",
    "\n",
    "이걸 Embedding layer에 먼저 넣어서 400 차원으로 축소시킴\n",
    "\n",
    "LSTM layer를 통해서 256개의 unit값\n",
    "\n",
    "Dropout layer로 30%는 꺼버리고\n",
    "\n",
    "마지막 FC layer를 통해서 1개의 값으로 만듦\n",
    "\n",
    "그 값을 Sigmoid로 압축"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78767118",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca02113",
   "metadata": {},
   "source": [
    "## 07. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb2fc3",
   "metadata": {},
   "source": [
    "### 7-1. Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2acaf7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903b7d6",
   "metadata": {},
   "source": [
    "### 7-2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d0c55dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time 측정용\n",
    "import time\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30c4cae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100step time: 234.29950976371765 [sec]\n",
      "Epoch: 1/4... Step: 100... Loss: 0.590224... Val Loss: 0.647421\n",
      "100step time: 384.726930141449 [sec]\n",
      "Epoch: 1/4... Step: 200... Loss: 0.575770... Val Loss: 0.580677\n",
      "100step time: 533.2854495048523 [sec]\n",
      "Epoch: 1/4... Step: 300... Loss: 0.646433... Val Loss: 0.593216\n",
      "100step time: 679.8651714324951 [sec]\n",
      "Epoch: 1/4... Step: 400... Loss: 0.566177... Val Loss: 0.587839\n",
      "100step time: 148.69187378883362 [sec]\n",
      "Epoch: 2/4... Step: 500... Loss: 0.697694... Val Loss: 0.692770\n",
      "100step time: 294.4412753582001 [sec]\n",
      "Epoch: 2/4... Step: 600... Loss: 0.705943... Val Loss: 0.690536\n",
      "100step time: 440.5910835266113 [sec]\n",
      "Epoch: 2/4... Step: 700... Loss: 0.526810... Val Loss: 0.617581\n",
      "100step time: 587.9177124500275 [sec]\n",
      "Epoch: 2/4... Step: 800... Loss: 0.404018... Val Loss: 0.513853\n",
      "100step time: 146.95251655578613 [sec]\n",
      "Epoch: 3/4... Step: 900... Loss: 0.324925... Val Loss: 0.486536\n",
      "100step time: 298.1655373573303 [sec]\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.482657... Val Loss: 0.499480\n",
      "100step time: 449.404620885849 [sec]\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.179004... Val Loss: 0.457892\n",
      "100step time: 601.9645080566406 [sec]\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.278049... Val Loss: 0.424395\n",
      "100step time: 146.77734994888306 [sec]\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.135590... Val Loss: 0.464717\n",
      "100step time: 301.32136392593384 [sec]\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.448255... Val Loss: 0.530353\n",
      "100step time: 457.2348895072937 [sec]\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.264398... Val Loss: 0.462013\n",
      "100step time: 601.8899168968201 [sec]\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.144182... Val Loss: 0.484087\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "\n",
    "# Move model to GPU, if available\n",
    "if (train_on_gpu):\n",
    "    net.cuda()\n",
    "    \n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    start = time()\n",
    "    # Initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:   # 50개의 batch 총 400번 = 20000\n",
    "        counter += 1\n",
    "        \n",
    "        # Move dataset to GPU, if available\n",
    "        if (train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            \n",
    "        # Hidden state를 새로운 변수에 복사 및 떼놓기\n",
    "        # .data를 통해서 requires_grad=False가 되면서 Hidden state가\n",
    "        # 굳이 '다시' 학습되지 않는다\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        # Zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # Get the output from the model\n",
    "        output, h = net(inputs,h)\n",
    "        \n",
    "        # Calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                if (train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                    \n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "               \n",
    "            end = time()\n",
    "            net.train()\n",
    "            print(f\"100step time: {end-start} [sec]\")\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59be2a7d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7fdfae",
   "metadata": {},
   "source": [
    "## 08. Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca75995",
   "metadata": {},
   "source": [
    "### 8-1. Testing with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5219564b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.48386398822069165\n",
      "Test Accuracy: 0.7936\n"
     ]
    }
   ],
   "source": [
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "# Init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:     # 50개의 batch 총 50번 = 2500\n",
    "    h = tuple([each.data for each in h])\n",
    "    \n",
    "    if (train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        \n",
    "    output, h = net(inputs,h)\n",
    "    \n",
    "    test_loss = criterion(output.squeeze(), labels.float())  # 매 Batch마다의 loss\n",
    "    test_losses.append(test_loss.item())                     # 누적시키기\n",
    "    \n",
    "    # output을 1또는 0으로 변환(가까운 값으로, 반올림?)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    # 정답(label)값과 반올림된것 비교\n",
    "    # tensor.eq(Tensor A, Tensor B) = 각 위치의 값이 같으면 True, 다르면 False\n",
    "    # 로 이루어진 Tensor 반환\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    \n",
    "    # Tensor를 NumPy Array 변환\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    # 맞춘값 누적\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "\n",
    "# Average Test loss\n",
    "print(f\"Test loss: {np.mean(test_losses)}\")\n",
    "\n",
    "# Accuracy over all test data\n",
    "test_acc = num_correct / len(test_loader.dataset)   # 2500, 즉 배치개수가 아닌 전체 데이터 개수\n",
    "    \n",
    "print(f\"Test Accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec732338",
   "metadata": {},
   "source": [
    "[+] 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d982f504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 400)\n",
       "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sig): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model save\n",
    "torch.save(net.state_dict(), \"SentimentRNN_epoch4.pt\")\n",
    "\n",
    "# Model load\n",
    "net2 = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "net2.load_state_dict(torch.load(\"SentimentRNN_epoch4.pt\"))\n",
    "net2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87170c4",
   "metadata": {},
   "source": [
    "### 8-2. Inference\n",
    "\n",
    "아래의 두가지 리뷰를 모델에 넣어서 어떻게 예측하는지 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "15ca4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review_neg = \"The worst movie I have seen; acting was terrible and I want my money back. This movie had bad acting and the dialogue was slow.\"\n",
    "                   \n",
    "test_review_pos = \"This movie had the best acting and the dialogue was so good. I loved it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01effda2",
   "metadata": {},
   "source": [
    "#### (1) 정제작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "055218d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the worst movie i have seen acting was terrible and i want my money back this movie had bad acting and the dialogue was slow\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "test_review = test_review_neg.lower()\n",
    "test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "print(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2930a1",
   "metadata": {},
   "source": [
    "#### (2) 텍스트 -> 단어리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3dba6326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'worst', 'movie', 'i', 'have', 'seen', 'acting', 'was', 'terrible', 'and', 'i', 'want', 'my', 'money', 'back', 'this', 'movie', 'had', 'bad', 'acting', 'and', 'the', 'dialogue', 'was', 'slow']\n"
     ]
    }
   ],
   "source": [
    "test_words = test_text.split()\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b90ed6",
   "metadata": {},
   "source": [
    "#### (3) 단어 -> 정수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff95d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ints = [vocab_to_int[word] for word in test_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e34a42",
   "metadata": {},
   "source": [
    "Batch, Review's' 차원으로 만들기 -> 우리가 `pad_features()` 함수를 이용하여 Padding을 진행할때 입력으로 받은 데이터는 Row는 리뷰개수, Column은 1개의 리뷰내의 단어개수의 차원이었다. 따라서 지금은 1개의 리뷰이지만 2차원으로 만들어줘야 패딩을 진행할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea88eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ints = [test_ints]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c2915",
   "metadata": {},
   "source": [
    "#### (1,2,3) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9e788353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower()\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "    test_words = test_text.split()\n",
    "    test_ints = [vocab_to_int[word] for word in test_words]\n",
    "    test_ints = [test_ints]\n",
    "    \n",
    "    return test_ints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4168d0ea",
   "metadata": {},
   "source": [
    "#### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2380b3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    '''\n",
    "    net : A Trained Network model\n",
    "    test_review : A review made of normal text and punctuation\n",
    "    sequence_length : The padded length of a review\n",
    "    '''\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    seq_length = sequence_length \n",
    "    \n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if (train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "        \n",
    "    output, h = net(feature_tensor, h)\n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    print(f\"Prediction value, (pre-rounding): {output.item()})\")\n",
    "    \n",
    "    if (pred.item() == 1):\n",
    "        print(\"Positive Review!\")\n",
    "    else:\n",
    "        print(\"Negative Review!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6b82dd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, (pre-rounding): 0.01452025305479765)\n",
      "Negative Review!\n"
     ]
    }
   ],
   "source": [
    "predict(net2, test_review_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4680644a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, (pre-rounding): 0.9390115737915039)\n",
      "Positive Review!\n"
     ]
    }
   ],
   "source": [
    "predict(net2, test_review_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cd008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
